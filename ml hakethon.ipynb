{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Set3_Sentiment/train/train_prop_sent_csv3_final.csv', encoding='ISO-8859-1')\n",
    "# Define path to video clips\n",
    "video_dir = 'Set3_Sentiment/train/train_prop_sent_data3'\n",
    "\n",
    "\n",
    "# Function to get video file path from IDs\n",
    "def get_video_clip_path(row):\n",
    "    dialogue_id = row['Dialogue_ID']\n",
    "    utterance_id = row['Utterance_ID']\n",
    "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
    "    return os.path.join(video_dir, filename)\n",
    "\n",
    "# Apply the function to get file paths for each sampled clip\n",
    "train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n",
    "\n",
    "# Check sample paths\n",
    "print(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text while preserving emotional markers\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace multiple exclamation/question marks with single ones while preserving them\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\?+', '?', text)\n",
    "    \n",
    "    # Handle ellipsis (preserve as single ...)\n",
    "    text = re.sub(r'\\.{2,}', '...', text)\n",
    "    \n",
    "    # Remove other punctuation except ! ? ...\n",
    "    def remove_punct(text):\n",
    "        punct = string.punctuation.replace('!', '').replace('?', '').replace('.', '')\n",
    "        return ''.join(ch for ch in text if ch not in punct)\n",
    "    \n",
    "    text = remove_punct(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Add cleaned text as new column\n",
    "train_df['cleaned_text'] = train_df['Utterance'].apply(clean_text)\n",
    "\n",
    "# Display some examples\n",
    "print(\"Original vs Cleaned Text Examples:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nOriginal: {train_df['Utterance'].iloc[i]}\")\n",
    "    print(f\"Cleaned : {train_df['cleaned_text'].iloc[i]}\")\n",
    "\n",
    "# Add emotion markers count as features\n",
    "train_df['exclamation_count'] = train_df['Utterance'].str.count(r'!')\n",
    "train_df['question_count'] = train_df['Utterance'].str.count(r'\\?')\n",
    "train_df['ellipsis_count'] = train_df['Utterance'].str.count(r'\\.\\.\\.')\n",
    "\n",
    "# Show distribution of emotion markers\n",
    "print(\"\\nEmotion Markers Statistics:\")\n",
    "print(\"\\nExclamation marks:\")\n",
    "print(train_df['exclamation_count'].value_counts().head())\n",
    "print(\"\\nQuestion marks:\")\n",
    "print(train_df['question_count'].value_counts().head())\n",
    "print(\"\\nEllipsis:\")\n",
    "print(train_df['ellipsis_count'].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get previous utterances in same dialogue\n",
    "def get_context_window(df, current_row, window_size=2):\n",
    "    \"\"\"\n",
    "    Get previous utterances in the same dialogue\n",
    "    \"\"\"\n",
    "    dialogue_id = current_row['Dialogue_ID']\n",
    "    utterance_id = current_row['Utterance_ID']\n",
    "    \n",
    "    # Get all utterances from same dialogue with lower utterance IDs\n",
    "    prev_utterances = df[\n",
    "        (df['Dialogue_ID'] == dialogue_id) & \n",
    "        (df['Utterance_ID'] < utterance_id)\n",
    "    ].sort_values('Utterance_ID', ascending=False)\n",
    "    \n",
    "    # Take the last 'window_size' utterances\n",
    "    return prev_utterances.head(window_size)\n",
    "\n",
    "# Add context features\n",
    "def add_context_features(df):\n",
    "    \"\"\"\n",
    "    Add contextual features to the dataframe\n",
    "    \"\"\"\n",
    "    # Initialize new columns\n",
    "    df['prev_sentiment'] = None\n",
    "    df['prev_speaker'] = None\n",
    "    df['time_gap'] = 0.0\n",
    "    df['utterance_position'] = 0\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get previous utterances\n",
    "        prev_utts = get_context_window(df, row)\n",
    "        \n",
    "        if not prev_utts.empty:\n",
    "            # Get previous sentiment\n",
    "            df.at[idx, 'prev_sentiment'] = prev_utts.iloc[0]['Sentiment']\n",
    "            \n",
    "            # Get previous speaker\n",
    "            df.at[idx, 'prev_speaker'] = prev_utts.iloc[0]['Speaker']\n",
    "            \n",
    "            # Calculate time gap\n",
    "            current_start = pd.to_datetime(row['StartTime'])\n",
    "            prev_end = pd.to_datetime(prev_utts.iloc[0]['EndTime'])\n",
    "            df.at[idx, 'time_gap'] = (current_start - prev_end).total_seconds()\n",
    "    \n",
    "    # Add dialogue position feature\n",
    "    dialogue_lengths = df.groupby('Dialogue_ID').size()\n",
    "    df['dialogue_length'] = df['Dialogue_ID'].map(dialogue_lengths)\n",
    "    df['utterance_position'] = df.groupby('Dialogue_ID')['Utterance_ID'].rank()\n",
    "    df['relative_position'] = df['utterance_position'] / df['dialogue_length']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply context features\n",
    "train_df = add_context_features(train_df)\n",
    "\n",
    "# Display example of context features\n",
    "print(\"\\nExample of context features:\")\n",
    "print(train_df[['Utterance', 'Speaker', 'Sentiment', \n",
    "                'prev_sentiment', 'prev_speaker', \n",
    "                'time_gap', 'relative_position']].head())\n",
    "\n",
    "# Analyze speaker interactions\n",
    "speaker_pairs = train_df[['Speaker', 'prev_speaker']].value_counts()\n",
    "print(\"\\nMost common speaker interactions:\")\n",
    "print(speaker_pairs.head())\n",
    "\n",
    "# Analyze sentiment transitions\n",
    "sentiment_transitions = train_df[['Sentiment', 'prev_sentiment']].value_counts()\n",
    "print(\"\\nMost common sentiment transitions:\")\n",
    "print(sentiment_transitions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create speaker profiles and analyze patterns\n",
    "def create_speaker_profiles(df):\n",
    "    # Get sentiment distribution for each speaker\n",
    "    speaker_sentiment = pd.crosstab(df['Speaker'], df['Sentiment'], normalize='index')\n",
    "    \n",
    "    # Calculate average emotion markers per speaker\n",
    "    speaker_stats = df.groupby('Speaker').agg({\n",
    "        'exclamation_count': 'mean',\n",
    "        'question_count': 'mean',\n",
    "        'ellipsis_count': 'mean',\n",
    "        'Utterance': 'count'  # number of utterances\n",
    "    }).rename(columns={'Utterance': 'total_utterances'})\n",
    "    \n",
    "    # Combine with sentiment distributions\n",
    "    speaker_profiles = pd.concat([speaker_sentiment, speaker_stats], axis=1)\n",
    "    \n",
    "    return speaker_profiles\n",
    "\n",
    "# Analyze character interactions\n",
    "def analyze_character_interactions(df):\n",
    "    # Create speaker pairs\n",
    "    df['speaker_pair'] = df.apply(lambda x: \n",
    "        tuple(sorted([x['Speaker'], x['prev_speaker']]))\n",
    "        if pd.notna(x['prev_speaker']) else None, axis=1)\n",
    "    \n",
    "    # Get sentiment patterns for each speaker pair\n",
    "    pair_sentiments = df[df['speaker_pair'].notna()].groupby('speaker_pair')['Sentiment'].value_counts()\n",
    "    \n",
    "    # Calculate average time gap between speakers\n",
    "    pair_time_gaps = df[df['speaker_pair'].notna()].groupby('speaker_pair')['time_gap'].mean()\n",
    "    \n",
    "    return pair_sentiments, pair_time_gaps\n",
    "\n",
    "# Create speaker profiles\n",
    "speaker_profiles = create_speaker_profiles(train_df)\n",
    "print(\"Speaker Profiles:\")\n",
    "print(speaker_profiles.round(3))\n",
    "\n",
    "# Analyze main character interactions\n",
    "pair_sentiments, pair_time_gaps = analyze_character_interactions(train_df)\n",
    "\n",
    "# Display specific character interactions\n",
    "main_characters = ['Ross', 'Rachel', 'Monica', 'Chandler', 'Joey', 'Phoebe']\n",
    "print(\"\\nMain Character Interactions:\")\n",
    "for pair in pair_sentiments.index.levels[0]:\n",
    "    if all(char in pair for char in ['Ross', 'Rachel']):\n",
    "        print(\"\\nRoss-Rachel Interactions:\")\n",
    "        print(pair_sentiments[pair])\n",
    "    elif all(char in pair for char in ['Monica', 'Chandler']):\n",
    "        print(\"\\nMonica-Chandler Interactions:\")\n",
    "        print(pair_sentiments[pair])\n",
    "\n",
    "# Add speaker-based features to dataframe\n",
    "def add_speaker_features(df, speaker_profiles):\n",
    "    # Add speaker's typical sentiment distributions\n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        col_name = f'speaker_{sentiment}_ratio'\n",
    "        df[col_name] = df['Speaker'].map(speaker_profiles[sentiment])\n",
    "    \n",
    "    # Add speaker's emotion marker averages\n",
    "    for marker in ['exclamation_count', 'question_count', 'ellipsis_count']:\n",
    "        col_name = f'speaker_avg_{marker}'\n",
    "        df[col_name] = df['Speaker'].map(speaker_profiles[marker])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply speaker features\n",
    "train_df = add_speaker_features(train_df, speaker_profiles)\n",
    "\n",
    "# Display example of new features\n",
    "print(\"\\nExample of speaker-based features:\")\n",
    "print(train_df[['Speaker', 'Sentiment', \n",
    "                'speaker_positive_ratio', 'speaker_negative_ratio', \n",
    "                'speaker_avg_exclamation_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract temporal features\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Extract temporal features from StartTime and EndTime\n",
    "    \"\"\"\n",
    "    # Convert time strings to datetime\n",
    "    df['start_time'] = pd.to_datetime(df['StartTime'], format='%H:%M:%S,%f')\n",
    "    df['end_time'] = pd.to_datetime(df['EndTime'], format='%H:%M:%S,%f')\n",
    "    \n",
    "    # Calculate utterance duration in seconds\n",
    "    df['utterance_duration'] = (df['end_time'] - df['start_time']).dt.total_seconds()\n",
    "    \n",
    "    # Calculate speaking rate (words per second)\n",
    "    df['word_count'] = df['Utterance'].str.split().str.len()\n",
    "    df['speaking_rate'] = df['word_count'] / df['utterance_duration']\n",
    "    \n",
    "    # Calculate pause after utterance (time until next utterance in same dialogue)\n",
    "    df['pause_after'] = 0.0\n",
    "    \n",
    "    for dialogue_id in df['Dialogue_ID'].unique():\n",
    "        dialogue_mask = df['Dialogue_ID'] == dialogue_id\n",
    "        dialogue_df = df[dialogue_mask].sort_values('Utterance_ID')\n",
    "        \n",
    "        # Calculate time difference to next utterance\n",
    "        df.loc[dialogue_mask, 'pause_after'] = (\n",
    "            dialogue_df['start_time'].shift(-1) - dialogue_df['end_time']\n",
    "        ).dt.total_seconds()\n",
    "    \n",
    "    # Identify rapid exchanges (short duration + short pause)\n",
    "    df['is_rapid_exchange'] = (\n",
    "        (df['utterance_duration'] < df['utterance_duration'].median()) & \n",
    "        (df['pause_after'] < df['pause_after'].median())\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply temporal features\n",
    "train_df = add_temporal_features(train_df)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Temporal Features Summary:\")\n",
    "print(\"\\nUtterance Duration Statistics (seconds):\")\n",
    "print(train_df['utterance_duration'].describe())\n",
    "\n",
    "print(\"\\nSpeaking Rate Statistics (words/second):\")\n",
    "print(train_df['speaking_rate'].describe())\n",
    "\n",
    "print(\"\\nPause Duration Statistics (seconds):\")\n",
    "print(train_df['pause_after'].describe())\n",
    "\n",
    "# Analyze relationship between temporal features and sentiment\n",
    "print(\"\\nAverage Duration by Sentiment:\")\n",
    "print(train_df.groupby('Sentiment')['utterance_duration'].mean())\n",
    "\n",
    "print(\"\\nAverage Speaking Rate by Sentiment:\")\n",
    "print(train_df.groupby('Sentiment')['speaking_rate'].mean())\n",
    "\n",
    "print(\"\\nPercentage of Rapid Exchanges by Sentiment:\")\n",
    "print(train_df.groupby('Sentiment')['is_rapid_exchange'].mean() * 100)\n",
    "\n",
    "# Visualize temporal patterns\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.boxplot([train_df[train_df['Sentiment'] == s]['utterance_duration'] \n",
    "             for s in ['positive', 'neutral', 'negative']], \n",
    "            labels=['Positive', 'Neutral', 'Negative'])\n",
    "plt.title('Duration by Sentiment')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.boxplot([train_df[train_df['Sentiment'] == s]['speaking_rate'] \n",
    "             for s in ['positive', 'neutral', 'negative']], \n",
    "            labels=['Positive', 'Neutral', 'Negative'])\n",
    "plt.title('Speaking Rate by Sentiment')\n",
    "plt.ylabel('Words per Second')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.boxplot([train_df[train_df['Sentiment'] == s]['pause_after'] \n",
    "             for s in ['positive', 'neutral', 'negative']], \n",
    "            labels=['Positive', 'Neutral', 'Negative'])\n",
    "plt.title('Pause Duration by Sentiment')\n",
    "plt.ylabel('Pause (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add episode context features\n",
    "def add_episode_features(df):\n",
    "    \"\"\"\n",
    "    Add features related to episode context and character development\n",
    "    \"\"\"\n",
    "    # Create episode identifier\n",
    "    df['episode_id'] = df['Season'].astype(str) + '_' + df['Episode'].astype(str)\n",
    "    \n",
    "    # Track character appearances per episode\n",
    "    episode_chars = df.groupby(['episode_id', 'Speaker']).size().reset_index()\n",
    "    episode_chars.columns = ['episode_id', 'Speaker', 'appearances_in_episode']\n",
    "    df = df.merge(episode_chars, on=['episode_id', 'Speaker'])\n",
    "    \n",
    "    # Calculate cumulative appearances for each character\n",
    "    df['cumulative_appearances'] = df.groupby('Speaker').cumcount() + 1\n",
    "    \n",
    "    # Track sentiment patterns across episodes\n",
    "    df['prev_episode_sentiment'] = None\n",
    "    for speaker in df['Speaker'].unique():\n",
    "        speaker_mask = df['Speaker'] == speaker\n",
    "        df.loc[speaker_mask, 'prev_episode_sentiment'] = (\n",
    "            df[speaker_mask].sort_values(['Season', 'Episode'])['Sentiment'].shift(1)\n",
    "        )\n",
    "    \n",
    "    # Calculate episode-level statistics\n",
    "    episode_stats = df.groupby('episode_id').agg({\n",
    "        'Sentiment': lambda x: x.value_counts().index[0],  # Most common sentiment\n",
    "        'Speaker': 'nunique',  # Number of unique speakers\n",
    "        'Dialogue_ID': 'nunique'  # Number of dialogues\n",
    "    }).reset_index()\n",
    "    episode_stats.columns = ['episode_id', 'episode_dominant_sentiment', \n",
    "                           'unique_speakers', 'dialogue_count']\n",
    "    df = df.merge(episode_stats, on='episode_id')\n",
    "    \n",
    "    # Track character interactions per episode\n",
    "    df['interaction_count'] = df.groupby(['episode_id', 'Speaker', 'prev_speaker']).cumcount() + 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply episode features\n",
    "train_df = add_episode_features(train_df)\n",
    "\n",
    "# Display summary of episode features\n",
    "print(\"\\nEpisode Context Analysis:\")\n",
    "print(\"\\nCharacter Appearances by Episode:\")\n",
    "print(train_df.groupby('episode_id')['Speaker'].value_counts().head())\n",
    "\n",
    "print(\"\\nDominant Sentiment by Episode:\")\n",
    "print(train_df.groupby('episode_id')['episode_dominant_sentiment'].first())\n",
    "\n",
    "print(\"\\nCharacter Development (Sample):\")\n",
    "for speaker in ['Ross', 'Rachel', 'Monica', 'Chandler', 'Joey', 'Phoebe']:\n",
    "    speaker_data = train_df[train_df['Speaker'] == speaker]\n",
    "    if not speaker_data.empty:\n",
    "        print(f\"\\n{speaker}:\")\n",
    "        print(f\"Total appearances: {len(speaker_data)}\")\n",
    "        print(\"Sentiment distribution:\")\n",
    "        print(speaker_data['Sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect text enhancement patterns\n",
    "def add_text_enhancement_features(df):\n",
    "    \"\"\"\n",
    "    Add features for detecting text patterns like shouting, repetition, stuttering, etc.\n",
    "    \"\"\"\n",
    "    # Initialize new columns\n",
    "    df['has_shouting'] = False\n",
    "    df['shouting_word_count'] = 0\n",
    "    df['has_repetition'] = False\n",
    "    df['repetition_count'] = 0\n",
    "    df['has_stuttering'] = False\n",
    "    df['stutter_count'] = 0\n",
    "    df['has_laughter'] = False\n",
    "    df['laughter_count'] = 0\n",
    "    \n",
    "    # Process each utterance\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['Utterance']\n",
    "        words = text.split()\n",
    "        \n",
    "        # 1. Detect shouting (words in ALL CAPS)\n",
    "        caps_words = [w for w in words if w.isupper() and len(w) > 1]\n",
    "        df.at[idx, 'has_shouting'] = len(caps_words) > 0\n",
    "        df.at[idx, 'shouting_word_count'] = len(caps_words)\n",
    "        \n",
    "        # 2. Detect repetition (words joined by hyphens)\n",
    "        repetitions = [w for w in words if w.count('-') >= 2]  # e.g., \"no-no-no\"\n",
    "        df.at[idx, 'has_repetition'] = len(repetitions) > 0\n",
    "        df.at[idx, 'repetition_count'] = len(repetitions)\n",
    "        \n",
    "        # 3. Detect stuttering (single letters/words with hyphens)\n",
    "        stutters = [w for w in words if len(w) <= 2 and '-' in w]  # e.g., \"I-I\"\n",
    "        df.at[idx, 'has_stuttering'] = len(stutters) > 0\n",
    "        df.at[idx, 'stutter_count'] = len(stutters)\n",
    "        \n",
    "        # 4. Detect laughter indicators\n",
    "        laughter_patterns = ['haha', 'hehe', 'lol', 'lmao']\n",
    "        found_laughter = []\n",
    "        for pattern in laughter_patterns:\n",
    "            if pattern in text.lower():\n",
    "                found_laughter.append(pattern)\n",
    "        df.at[idx, 'has_laughter'] = len(found_laughter) > 0\n",
    "        df.at[idx, 'laughter_count'] = len(found_laughter)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply text enhancement features\n",
    "train_df = add_text_enhancement_features(train_df)\n",
    "\n",
    "# Display examples and statistics\n",
    "print(\"Text Enhancement Examples:\")\n",
    "print(\"\\nShouting Examples:\")\n",
    "print(train_df[train_df['has_shouting']][['Utterance', 'shouting_word_count']].head())\n",
    "\n",
    "print(\"\\nRepetition Examples:\")\n",
    "print(train_df[train_df['has_repetition']][['Utterance', 'repetition_count']].head())\n",
    "\n",
    "print(\"\\nStuttering Examples:\")\n",
    "print(train_df[train_df['has_stuttering']][['Utterance', 'stutter_count']].head())\n",
    "\n",
    "print(\"\\nLaughter Examples:\")\n",
    "print(train_df[train_df['has_laughter']][['Utterance', 'laughter_count']].head())\n",
    "\n",
    "# Analyze relationship with sentiment\n",
    "print(\"\\nFeature Distribution by Sentiment:\")\n",
    "for feature in ['has_shouting', 'has_repetition', 'has_stuttering', 'has_laughter']:\n",
    "    print(f\"\\n{feature} distribution:\")\n",
    "    print(train_df.groupby('Sentiment')[feature].mean())\n",
    "\n",
    "# Visualize patterns\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Text enhancement features by sentiment\n",
    "features = ['has_shouting', 'has_repetition', 'has_stuttering', 'has_laughter']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "\n",
    "plt.subplot(131)\n",
    "for i, feature in enumerate(features):\n",
    "    feature_means = [train_df[train_df['Sentiment'] == s][feature].mean() for s in sentiments]\n",
    "    plt.bar([x + i*0.25 for x in range(len(sentiments))], feature_means, width=0.25, label=feature)\n",
    "\n",
    "plt.xticks([x + 0.375 for x in range(len(sentiments))], sentiments)\n",
    "plt.title('Text Enhancement Features by Sentiment')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(\"\\nPercentage of utterances with:\")\n",
    "for feature in ['has_shouting', 'has_repetition', 'has_stuttering', 'has_laughter']:\n",
    "    percentage = (train_df[feature].sum() / len(train_df)) * 100\n",
    "    print(f\"{feature}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze dialogue structure\n",
    "def add_dialogue_structure_features(df):\n",
    "    \"\"\"\n",
    "    Add features related to dialogue structure and patterns\n",
    "    \"\"\"\n",
    "    # Initialize new columns\n",
    "    df['is_question'] = df['Utterance'].str.contains(r'\\?')\n",
    "    df['is_answer'] = False\n",
    "    df['is_interruption'] = False\n",
    "    df['conversation_size'] = 0\n",
    "    df['is_opening'] = False\n",
    "    df['is_closing'] = False\n",
    "    \n",
    "    # Process each dialogue\n",
    "    for dialogue_id in df['Dialogue_ID'].unique():\n",
    "        dialogue_mask = df['Dialogue_ID'] == dialogue_id\n",
    "        dialogue = df[dialogue_mask].sort_values('Utterance_ID')\n",
    "        \n",
    "        # Get conversation size (number of unique speakers)\n",
    "        conversation_size = dialogue['Speaker'].nunique()\n",
    "        df.loc[dialogue_mask, 'conversation_size'] = conversation_size\n",
    "        \n",
    "        # Mark answers (utterances following questions)\n",
    "        questions = dialogue[dialogue['is_question']].index\n",
    "        for q_idx in questions:\n",
    "            next_utt_idx = dialogue.index[dialogue.index.get_loc(q_idx) + 1] if q_idx != dialogue.index[-1] else None\n",
    "            if next_utt_idx is not None:\n",
    "                df.at[next_utt_idx, 'is_answer'] = True\n",
    "        \n",
    "        # Detect interruptions (short time gaps or overlapping timestamps)\n",
    "        time_gaps = pd.to_datetime(dialogue['StartTime']) - pd.to_datetime(dialogue['EndTime']).shift(1)\n",
    "        interruption_mask = (time_gaps.dt.total_seconds() < 0.5) & (time_gaps.dt.total_seconds() > 0)\n",
    "        df.loc[dialogue[interruption_mask].index, 'is_interruption'] = True\n",
    "        \n",
    "        # Mark opening/closing utterances\n",
    "        if len(dialogue) > 0:\n",
    "            df.loc[dialogue.index[0], 'is_opening'] = True\n",
    "            df.loc[dialogue.index[-1], 'is_closing'] = True\n",
    "    \n",
    "    # Add features for conversation type\n",
    "    df['is_one_on_one'] = df['conversation_size'] == 2\n",
    "    df['is_group_chat'] = df['conversation_size'] > 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply dialogue structure features\n",
    "train_df = add_dialogue_structure_features(train_df)\n",
    "\n",
    "# Display examples and statistics\n",
    "print(\"Dialogue Structure Analysis:\")\n",
    "print(\"\\nQuestion-Answer Examples:\")\n",
    "qa_pairs = train_df[train_df['is_question'] | train_df['is_answer']][\n",
    "    ['Utterance', 'Speaker', 'is_question', 'is_answer']\n",
    "].head()\n",
    "print(qa_pairs)\n",
    "\n",
    "print(\"\\nInterruption Examples:\")\n",
    "interruptions = train_df[train_df['is_interruption']][\n",
    "    ['Utterance', 'Speaker', 'StartTime', 'EndTime']\n",
    "].head()\n",
    "print(interruptions)\n",
    "\n",
    "print(\"\\nConversation Size Distribution:\")\n",
    "print(train_df['conversation_size'].value_counts())\n",
    "\n",
    "print(\"\\nConversation Type Statistics:\")\n",
    "print(\"One-on-one conversations:\", (train_df['is_one_on_one']).sum())\n",
    "print(\"Group conversations:\", (train_df['is_group_chat']).sum())\n",
    "\n",
    "# Analyze relationship with sentiment\n",
    "print(\"\\nSentiment Distribution by Dialogue Structure:\")\n",
    "for feature in ['is_question', 'is_answer', 'is_interruption', 'is_opening', 'is_closing']:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(train_df.groupby([feature, 'Sentiment']).size().unstack(fill_value=0))\n",
    "\n",
    "# Visualize patterns\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Conversation sizes\n",
    "plt.subplot(131)\n",
    "train_df['conversation_size'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Conversation Sizes')\n",
    "plt.xlabel('Number of Participants')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot 2: Dialogue structure features by sentiment\n",
    "plt.subplot(132)\n",
    "features = ['is_question', 'is_answer', 'is_interruption']\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    feature_means = [train_df[train_df['Sentiment'] == sentiment][f].mean() for f in features]\n",
    "    plt.bar([x + ['pos', 'neu', 'neg'].index(sentiment[0:3])*0.25 for x in range(len(features))],\n",
    "            feature_means, width=0.25, label=sentiment)\n",
    "plt.xticks(range(len(features)), features, rotation=45)\n",
    "plt.title('Dialogue Features by Sentiment')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Opening/Closing sentiment distribution\n",
    "plt.subplot(133)\n",
    "opening_closing = pd.DataFrame({\n",
    "    'Opening': train_df[train_df['is_opening']]['Sentiment'].value_counts(),\n",
    "    'Closing': train_df[train_df['is_closing']]['Sentiment'].value_counts()\n",
    "})\n",
    "opening_closing.plot(kind='bar')\n",
    "plt.title('Sentiment in Opening/Closing Utterances')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: Speaker transitions\n",
    "def analyze_speaker_transitions(df):\n",
    "    \"\"\"\n",
    "    Analyze how speakers transition in conversations\n",
    "    \"\"\"\n",
    "    transitions = []\n",
    "    for dialogue_id in df['Dialogue_ID'].unique():\n",
    "        dialogue = df[df['Dialogue_ID'] == dialogue_id].sort_values('Utterance_ID')\n",
    "        speakers = dialogue['Speaker'].tolist()\n",
    "        for i in range(len(speakers)-1):\n",
    "            transitions.append((speakers[i], speakers[i+1]))\n",
    "    \n",
    "    return pd.DataFrame(transitions, columns=['from_speaker', 'to_speaker'])\n",
    "\n",
    "speaker_transitions = analyze_speaker_transitions(train_df)\n",
    "print(\"\\nMost Common Speaker Transitions:\")\n",
    "print(speaker_transitions.value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect emotion intensifiers\n",
    "def add_emotion_intensifier_features(df):\n",
    "    \"\"\"\n",
    "    Add features for detecting emotion intensifiers like repeated punctuation,\n",
    "    emphasis words, and emotion words\n",
    "    \"\"\"\n",
    "    # Initialize new columns\n",
    "    df['has_repeated_punct'] = False\n",
    "    df['repeated_punct_count'] = 0\n",
    "    df['has_emphasis_words'] = False\n",
    "    df['emphasis_word_count'] = 0\n",
    "    df['has_emotion_words'] = False\n",
    "    df['emotion_word_count'] = 0\n",
    "    \n",
    "    # Define patterns\n",
    "    emphasis_words = ['very', 'so', 'really', 'extremely', 'totally', 'absolutely', \n",
    "                     'completely', 'literally', 'definitely', 'seriously']\n",
    "    \n",
    "    positive_emotions = ['love', 'happy', 'excited', 'glad', 'wonderful', 'great',\n",
    "                        'amazing', 'fantastic', 'awesome', 'excellent', 'perfect']\n",
    "    \n",
    "    negative_emotions = ['hate', 'angry', 'sad', 'upset', 'terrible', 'horrible',\n",
    "                        'awful', 'furious', 'annoyed', 'disappointed', 'worried']\n",
    "    \n",
    "    # Process each utterance\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['Utterance'].lower()\n",
    "        \n",
    "        # 1. Check for repeated punctuation\n",
    "        repeated_punct = re.findall(r'[!?]{2,}', text)\n",
    "        df.at[idx, 'has_repeated_punct'] = len(repeated_punct) > 0\n",
    "        df.at[idx, 'repeated_punct_count'] = len(repeated_punct)\n",
    "        \n",
    "        # 2. Check for emphasis words\n",
    "        emphasis_count = sum(1 for word in emphasis_words if word in text.split())\n",
    "        df.at[idx, 'has_emphasis_words'] = emphasis_count > 0\n",
    "        df.at[idx, 'emphasis_word_count'] = emphasis_count\n",
    "        \n",
    "        # 3. Check for emotion words\n",
    "        emotion_count = sum(1 for word in positive_emotions + negative_emotions \n",
    "                          if word in text.split())\n",
    "        df.at[idx, 'has_emotion_words'] = emotion_count > 0\n",
    "        df.at[idx, 'emotion_word_count'] = emotion_count\n",
    "        \n",
    "        # Additional: Track positive vs negative emotions separately\n",
    "        df.at[idx, 'positive_emotion_count'] = sum(1 for word in positive_emotions \n",
    "                                                  if word in text.split())\n",
    "        df.at[idx, 'negative_emotion_count'] = sum(1 for word in negative_emotions \n",
    "                                                  if word in text.split())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply emotion intensifier features\n",
    "train_df = add_emotion_intensifier_features(train_df)\n",
    "\n",
    "# Display examples and statistics\n",
    "print(\"Emotion Intensifier Examples:\")\n",
    "print(\"\\nRepeated Punctuation Examples:\")\n",
    "print(train_df[train_df['has_repeated_punct']][['Utterance', 'repeated_punct_count']].head())\n",
    "\n",
    "print(\"\\nEmphasis Word Examples:\")\n",
    "print(train_df[train_df['has_emphasis_words']][['Utterance', 'emphasis_word_count']].head())\n",
    "\n",
    "print(\"\\nEmotion Word Examples:\")\n",
    "print(train_df[train_df['has_emotion_words']][['Utterance', 'emotion_word_count', \n",
    "                                              'positive_emotion_count', \n",
    "                                              'negative_emotion_count']].head())\n",
    "\n",
    "# Analyze relationship with sentiment\n",
    "print(\"\\nFeature Distribution by Sentiment:\")\n",
    "for feature in ['has_repeated_punct', 'has_emphasis_words', 'has_emotion_words']:\n",
    "    print(f\"\\n{feature} distribution:\")\n",
    "    print(train_df.groupby('Sentiment')[feature].mean())\n",
    "\n",
    "# Visualize patterns\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Emotion intensifier features by sentiment\n",
    "features = ['has_repeated_punct', 'has_emphasis_words', 'has_emotion_words']\n",
    "sentiments = ['positive', 'neutral', 'negative']\n",
    "\n",
    "plt.subplot(131)\n",
    "for i, feature in enumerate(features):\n",
    "    feature_means = [train_df[train_df['Sentiment'] == s][feature].mean() \n",
    "                    for s in sentiments]\n",
    "    plt.bar([x + i*0.25 for x in range(len(sentiments))], feature_means, \n",
    "            width=0.25, label=feature)\n",
    "\n",
    "plt.xticks([x + 0.25 for x in range(len(sentiments))], sentiments)\n",
    "plt.title('Emotion Intensifiers by Sentiment')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "# Plot 2: Positive vs Negative emotion words by sentiment\n",
    "plt.subplot(132)\n",
    "pos_means = [train_df[train_df['Sentiment'] == s]['positive_emotion_count'].mean() \n",
    "             for s in sentiments]\n",
    "neg_means = [train_df[train_df['Sentiment'] == s]['negative_emotion_count'].mean() \n",
    "             for s in sentiments]\n",
    "\n",
    "x = range(len(sentiments))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x], pos_means, width, label='Positive Emotions')\n",
    "plt.bar([i + width/2 for i in x], neg_means, width, label='Negative Emotions')\n",
    "plt.xticks(x, sentiments)\n",
    "plt.title('Emotion Words by Sentiment')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(\"\\nPercentage of utterances with:\")\n",
    "for feature in ['has_repeated_punct', 'has_emphasis_words', 'has_emotion_words']:\n",
    "    percentage = (train_df[feature].sum() / len(train_df)) * 100\n",
    "    print(f\"{feature}: {percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nAverage counts per utterance:\")\n",
    "for feature in ['repeated_punct_count', 'emphasis_word_count', 'emotion_word_count']:\n",
    "    mean = train_df[feature].mean()\n",
    "    print(f\"{feature}: {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.video Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_features(df):\n",
    "    \"\"\"Add advanced features while keeping existing ones\"\"\"\n",
    "    # Text length features\n",
    "    df['word_count'] = df['Utterance'].str.split().str.len()\n",
    "    df['char_count'] = df['Utterance'].str.len()\n",
    "    df['avg_word_length'] = df['char_count'] / df['word_count']\n",
    "    \n",
    "    # Emotional markers\n",
    "    df['has_caps'] = df['Utterance'].str.contains(r'[A-Z]{2,}').astype(int)\n",
    "    df['has_repeated_chars'] = df['Utterance'].str.contains(r'(.)\\1{2,}').astype(int)\n",
    "    df['has_emphasis'] = df['Utterance'].str.contains(r'!+|\\?+|\\.{2,}').astype(int)\n",
    "    \n",
    "    # Dialogue context\n",
    "    df['is_first_utterance'] = df.groupby('Dialogue_ID')['Utterance_ID'].transform('min') == df['Utterance_ID']\n",
    "    df['is_last_utterance'] = df.groupby('Dialogue_ID')['Utterance_ID'].transform('max') == df['Utterance_ID']\n",
    "    \n",
    "    # Speaker patterns\n",
    "    df['speaker_turn_length'] = df.groupby(['Dialogue_ID', 'Speaker']).cumcount()\n",
    "    df['speaker_change'] = (df['Speaker'] != df['Speaker'].shift()).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    \"\"\"Add features about speaker interactions\"\"\"\n",
    "    # Previous speaker sentiment\n",
    "    df['prev_speaker_sentiment'] = df.groupby('Dialogue_ID')['Sentiment'].shift(1)\n",
    "    \n",
    "    # Count of previous interactions between speakers\n",
    "    df['speaker_pair'] = df.apply(lambda x: '_'.join(sorted([x['Speaker'], str(x['prev_speaker'])])), axis=1)\n",
    "    df['interaction_count'] = df.groupby('speaker_pair').cumcount()\n",
    "    \n",
    "    # Speaker's emotional tendency\n",
    "    speaker_sentiment = df.groupby('Speaker')['Sentiment'].value_counts(normalize=True).unstack()\n",
    "    df['speaker_positive_ratio'] = df['Speaker'].map(speaker_sentiment['positive'].fillna(0))\n",
    "    df['speaker_negative_ratio'] = df['Speaker'].map(speaker_sentiment['negative'].fillna(0))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_dataset(df):\n",
    "    \"\"\"Balance the dataset using upsampling\"\"\"\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df['Sentiment'] == 'neutral']\n",
    "    df_minority_pos = df[df['Sentiment'] == 'positive']\n",
    "    df_minority_neg = df[df['Sentiment'] == 'negative']\n",
    "    \n",
    "    # Upsample minority classes\n",
    "    df_minority_pos_upsampled = resample(df_minority_pos,\n",
    "                                       replace=True,\n",
    "                                       n_samples=len(df_majority),\n",
    "                                       random_state=42)\n",
    "    df_minority_neg_upsampled = resample(df_minority_neg,\n",
    "                                       replace=True,\n",
    "                                       n_samples=len(df_majority),\n",
    "                                       random_state=42)\n",
    "    \n",
    "    # Combine all samples\n",
    "    df_balanced = pd.concat([df_majority, \n",
    "                           df_minority_pos_upsampled,\n",
    "                           df_minority_neg_upsampled])\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# ! pip install opencv-python mtcnn tensorflow numpy\n",
    "\n",
    "# # Import required libraries\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from mtcnn import MTCNN\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Initialize MTCNN detector\n",
    "# detector = MTCNN()\n",
    "\n",
    "# # Function to extract frames from video\n",
    "# def extract_frames(video_path, sample_rate=0.5):\n",
    "#     \"\"\"\n",
    "#     Extract frames from video at given sample rate\n",
    "#     Args:\n",
    "#         video_path: Path to video file\n",
    "#         sample_rate: Extract frame every sample_rate seconds\n",
    "#     Returns:\n",
    "#         frames: List of extracted frames\n",
    "#     \"\"\"\n",
    "#     frames = []\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     frame_interval = int(fps * sample_rate)\n",
    "    \n",
    "#     frame_count = 0\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "            \n",
    "#         if frame_count % frame_interval == 0:\n",
    "#             # Convert BGR to RGB\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             frames.append(frame_rgb)\n",
    "            \n",
    "#         frame_count += 1\n",
    "        \n",
    "#     cap.release()\n",
    "#     return frames\n",
    "\n",
    "# # Function to detect faces and extract features\n",
    "# def extract_face_features(frame):\n",
    "#     \"\"\"\n",
    "#     Detect faces and extract features using MTCNN\n",
    "#     Args:\n",
    "#         frame: Input image frame\n",
    "#     Returns:\n",
    "#         features: Dictionary containing facial features\n",
    "#     \"\"\"\n",
    "#     # Detect faces\n",
    "#     faces = detector.detect_faces(frame)\n",
    "    \n",
    "#     if len(faces) == 0:\n",
    "#         return None\n",
    "        \n",
    "#     # Get the largest face\n",
    "#     face = max(faces, key=lambda x: x['box'][2] * x['box'][3])\n",
    "    \n",
    "#     features = {\n",
    "#         'confidence': face['confidence'],\n",
    "#         'box': face['box'],\n",
    "#         'keypoints': face['keypoints'],\n",
    "#         'emotions': {\n",
    "#             'left_eye': face['keypoints']['left_eye'],\n",
    "#             'right_eye': face['keypoints']['right_eye'],\n",
    "#             'nose': face['keypoints']['nose'],\n",
    "#             'mouth_left': face['keypoints']['mouth_left'], \n",
    "#             'mouth_right': face['keypoints']['mouth_right']\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # Function to process video and extract features\n",
    "# def process_video(video_path):\n",
    "#     \"\"\"\n",
    "#     Process video file and extract facial features\n",
    "#     Args:\n",
    "#         video_path: Path to video file\n",
    "#     Returns:\n",
    "#         video_features: Dictionary containing features for each frame\n",
    "#     \"\"\"\n",
    "#     # Extract frames\n",
    "#     frames = extract_frames(video_path)\n",
    "    \n",
    "#     # Extract features from each frame\n",
    "#     video_features = []\n",
    "#     for frame in frames:\n",
    "#         features = extract_face_features(frame)\n",
    "#         if features is not None:\n",
    "#             video_features.append(features)\n",
    "            \n",
    "#     return video_features\n",
    "\n",
    "# # Test on a sample video\n",
    "# video_path = train_df['video_clip_path'][0]\n",
    "# features = process_video(video_path)\n",
    "\n",
    "# print(f\"Extracted features from {len(features)} frames\")\n",
    "# print(\"\\nSample features from first frame:\")\n",
    "# print(f\"Face confidence: {features[0]['confidence']:.2f}\")\n",
    "# print(f\"Face box: {features[0]['box']}\")\n",
    "# print(f\"Facial keypoints: {features[0]['keypoints']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install opencv-python\n",
    "%pip install mtcnn\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialize MTCNN detector\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, sample_rate=0.5):\n",
    "    \"\"\"\n",
    "    Extract frames from video at given sample rate\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        sample_rate: Extract frame every sample_rate seconds\n",
    "    Returns:\n",
    "        frames: List of extracted frames\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(fps * sample_rate)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, sample_rate=0.5):\n",
    "    \"\"\"\n",
    "    Extract frames from video at given sample rate\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        sample_rate: Extract frame every sample_rate seconds\n",
    "    Returns:\n",
    "        frames: List of extracted frames\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(fps * sample_rate)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_features(frame):\n",
    "    \"\"\"\n",
    "    Detect faces and extract features using MTCNN\n",
    "    Args:\n",
    "        frame: Input image frame\n",
    "    Returns:\n",
    "        features: Dictionary containing facial features\n",
    "    \"\"\"\n",
    "    # Detect faces\n",
    "    faces = detector.detect_faces(frame)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "        \n",
    "    # Get the largest face\n",
    "    face = max(faces, key=lambda x: x['box'][2] * x['box'][3])\n",
    "    \n",
    "    # Calculate geometric features\n",
    "    box = face['box']\n",
    "    keypoints = face['keypoints']\n",
    "    \n",
    "    # Calculate eye aspect ratio\n",
    "    left_eye = keypoints['left_eye']\n",
    "    right_eye = keypoints['right_eye']\n",
    "    eye_distance = np.sqrt((left_eye[0] - right_eye[0])**2 + \n",
    "                          (left_eye[1] - right_eye[1])**2)\n",
    "    \n",
    "    # Calculate mouth openness\n",
    "    mouth_left = keypoints['mouth_left']\n",
    "    mouth_right = keypoints['mouth_right']\n",
    "    mouth_width = np.sqrt((mouth_left[0] - mouth_right[0])**2 + \n",
    "                         (mouth_left[1] - mouth_right[1])**2)\n",
    "    \n",
    "    features = {\n",
    "        'confidence': face['confidence'],\n",
    "        'box': box,\n",
    "        'keypoints': keypoints,\n",
    "        'geometric_features': {\n",
    "            'eye_distance': eye_distance,\n",
    "            'mouth_width': mouth_width,\n",
    "            'face_height': box[3],\n",
    "            'face_width': box[2]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    \"\"\"\n",
    "    Process video file and extract facial features with temporal information\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "    Returns:\n",
    "        video_features: Dictionary containing features across frames\n",
    "    \"\"\"\n",
    "    # Extract frames\n",
    "    frames = extract_frames(video_path)\n",
    "    \n",
    "    # Extract features from each frame\n",
    "    frame_features = []\n",
    "    for frame in frames:\n",
    "        features = extract_face_features(frame)\n",
    "        if features is not None:\n",
    "            frame_features.append(features)\n",
    "    \n",
    "    # Calculate temporal features\n",
    "    if len(frame_features) > 1:\n",
    "        temporal_features = {\n",
    "            'confidence_mean': np.mean([f['confidence'] for f in frame_features]),\n",
    "            'face_movement': calculate_movement(frame_features),\n",
    "            'expression_change': calculate_expression_change(frame_features)\n",
    "        }\n",
    "    else:\n",
    "        temporal_features = None\n",
    "        \n",
    "    video_features = {\n",
    "        'frame_features': frame_features,\n",
    "        'temporal_features': temporal_features,\n",
    "        'num_frames': len(frame_features)\n",
    "    }\n",
    "            \n",
    "    return video_features\n",
    "\n",
    "def calculate_movement(frame_features):\n",
    "    \"\"\"Calculate face movement across frames\"\"\"\n",
    "    movements = []\n",
    "    for i in range(1, len(frame_features)):\n",
    "        prev_box = frame_features[i-1]['box']\n",
    "        curr_box = frame_features[i]['box']\n",
    "        \n",
    "        # Calculate center point movement\n",
    "        prev_center = [prev_box[0] + prev_box[2]/2, prev_box[1] + prev_box[3]/2]\n",
    "        curr_center = [curr_box[0] + curr_box[2]/2, curr_box[1] + curr_box[3]/2]\n",
    "        \n",
    "        movement = np.sqrt((prev_center[0] - curr_center[0])**2 + \n",
    "                         (prev_center[1] - curr_center[1])**2)\n",
    "        movements.append(movement)\n",
    "    \n",
    "    return np.mean(movements) if movements else 0\n",
    "\n",
    "def calculate_expression_change(frame_features):\n",
    "    \"\"\"Calculate expression changes across frames\"\"\"\n",
    "    changes = []\n",
    "    for i in range(1, len(frame_features)):\n",
    "        prev_features = frame_features[i-1]['geometric_features']\n",
    "        curr_features = frame_features[i]['geometric_features']\n",
    "        \n",
    "        # Calculate changes in key measurements\n",
    "        mouth_change = abs(prev_features['mouth_width'] - curr_features['mouth_width'])\n",
    "        eye_change = abs(prev_features['eye_distance'] - curr_features['eye_distance'])\n",
    "        \n",
    "        change = mouth_change + eye_change\n",
    "        changes.append(change)\n",
    "    \n",
    "    return np.mean(changes) if changes else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to organize features from video processing\n",
    "def organize_video_features(video_features):\n",
    "    \"\"\"\n",
    "    Organize raw video features into structured format\n",
    "    Args:\n",
    "        video_features: Dictionary containing raw features from video processing\n",
    "    Returns:\n",
    "        organized_features: Dictionary with structured features\n",
    "    \"\"\"\n",
    "    # Extract per-frame features\n",
    "    frame_features = video_features['frame_features']\n",
    "    \n",
    "    # Calculate per-frame metrics\n",
    "    per_frame_features = {\n",
    "        'confidence_scores': [f['confidence'] for f in frame_features],\n",
    "        'facial_landmarks': [f['keypoints'] for f in frame_features],\n",
    "        'geometric_features': [f['geometric_features'] for f in frame_features]\n",
    "    }\n",
    "    \n",
    "    # Calculate video-level features\n",
    "    video_level_features = {\n",
    "        'avg_confidence': np.mean(per_frame_features['confidence_scores']),\n",
    "        'expression_intensity': calculate_expression_intensity(frame_features),\n",
    "        'movement_amount': video_features['temporal_features']['face_movement'],\n",
    "        'expression_changes': video_features['temporal_features']['expression_change']\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'per_frame': per_frame_features,\n",
    "        'video_level': video_level_features\n",
    "    }\n",
    "\n",
    "def calculate_expression_intensity(frame_features):\n",
    "    \"\"\"Calculate overall expression intensity from geometric features\"\"\"\n",
    "    intensities = []\n",
    "    for frame in frame_features:\n",
    "        geom = frame['geometric_features']\n",
    "        # Combine mouth and eye movements as expression intensity\n",
    "        intensity = (geom['mouth_width'] / geom['face_width'] + \n",
    "                    geom['eye_distance'] / geom['face_width']) / 2\n",
    "        intensities.append(intensity)\n",
    "    return np.mean(intensities)\n",
    "\n",
    "# Function to combine video and text features\n",
    "def combine_features(video_features, text_features):\n",
    "    \"\"\"\n",
    "    Combine video and text features with temporal alignment\n",
    "    Args:\n",
    "        video_features: Dictionary containing video features\n",
    "        text_features: Dictionary containing text features\n",
    "    Returns:\n",
    "        combined_features: Dictionary with aligned multimodal features\n",
    "    \"\"\"\n",
    "    # Get utterance duration\n",
    "    duration = text_features['utterance_duration']\n",
    "    \n",
    "    # Normalize temporal features to utterance duration\n",
    "    normalized_video_features = {\n",
    "        'expression_intensity': video_features['video_level']['expression_intensity'],\n",
    "        'movement_amount': video_features['video_level']['movement_amount'] / duration,\n",
    "        'expression_changes': video_features['video_level']['expression_changes'] / duration\n",
    "    }\n",
    "    \n",
    "    # Combine with text features\n",
    "    combined_features = {\n",
    "        # Video features\n",
    "        'visual_confidence': video_features['video_level']['avg_confidence'],\n",
    "        'expression_intensity': normalized_video_features['expression_intensity'],\n",
    "        'movement_amount': normalized_video_features['movement_amount'],\n",
    "        'expression_changes': normalized_video_features['expression_changes'],\n",
    "        \n",
    "        # Text features\n",
    "        'text_sentiment': text_features['sentiment'],\n",
    "        'speaking_rate': text_features['speaking_rate'],\n",
    "        'utterance_duration': text_features['utterance_duration'],\n",
    "        \n",
    "        # Temporal features\n",
    "        'pause_after': text_features['pause_after'],\n",
    "        'is_rapid_exchange': text_features['is_rapid_exchange']\n",
    "    }\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# Process videos and combine features\n",
    "def process_all_videos(df):\n",
    "    \"\"\"\n",
    "    Process all videos and combine features\n",
    "    Args:\n",
    "        df: DataFrame containing video paths and text features\n",
    "    Returns:\n",
    "        all_features: List of combined features for each video\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Process video\n",
    "        video_path = row['video_clip_path']\n",
    "        video_features = process_video(video_path)\n",
    "        \n",
    "        # Skip if face detection failed\n",
    "        if video_features['num_frames'] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Organize video features\n",
    "        organized_video_features = organize_video_features(video_features)\n",
    "        \n",
    "        # Get text features\n",
    "        text_features = {\n",
    "            'sentiment': row['Sentiment'],\n",
    "            'speaking_rate': row['speaking_rate'],\n",
    "            'utterance_duration': row['utterance_duration'],\n",
    "            'pause_after': row['pause_after'],\n",
    "            'is_rapid_exchange': row['is_rapid_exchange']\n",
    "        }\n",
    "        \n",
    "        # Combine features\n",
    "        combined = combine_features(organized_video_features, text_features)\n",
    "        all_features.append(combined)\n",
    "        \n",
    "        # Print progress\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx} videos...\")\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# Example usage:\n",
    "# Process first few videos as example\n",
    "sample_df = train_df.head(5)\n",
    "sample_features = process_all_videos(sample_df)\n",
    "\n",
    "# Display example of combined features\n",
    "print(\"\\nExample of combined features:\")\n",
    "if sample_features:\n",
    "    for key, value in sample_features[0].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No features extracted. Please check your video processing pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoProcessor:\n",
    "    def __init__(self, sample_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initialize video processor\n",
    "        Args:\n",
    "            sample_rate: Extract frame every sample_rate seconds\n",
    "        \"\"\"\n",
    "        self.sample_rate = sample_rate\n",
    "        self.detector = MTCNN()  # Initialize MTCNN detector\n",
    "        \n",
    "    def load_video(self, video_path):\n",
    "        \"\"\"Load video and extract frames\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_interval = int(fps * self.sample_rate)\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % frame_interval == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def detect_faces(self, frame):\n",
    "        \"\"\"Detect faces and extract features\"\"\"\n",
    "        faces = self.detector.detect_faces(frame)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "            \n",
    "        # Get largest face\n",
    "        face = max(faces, key=lambda x: x['box'][2] * x['box'][3])\n",
    "        \n",
    "        # Extract geometric features\n",
    "        box = face['box']\n",
    "        keypoints = face['keypoints']\n",
    "        \n",
    "        # Calculate facial measurements\n",
    "        eye_distance = self.calculate_eye_distance(keypoints)\n",
    "        mouth_width = self.calculate_mouth_width(keypoints)\n",
    "        \n",
    "        features = {\n",
    "            'confidence': face['confidence'],\n",
    "            'box': box,\n",
    "            'keypoints': keypoints,\n",
    "            'measurements': {\n",
    "                'eye_distance': eye_distance,\n",
    "                'mouth_width': mouth_width,\n",
    "                'face_height': box[3],\n",
    "                'face_width': box[2]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def calculate_eye_distance(self, keypoints):\n",
    "        \"\"\"Calculate distance between eyes\"\"\"\n",
    "        left_eye = keypoints['left_eye']\n",
    "        right_eye = keypoints['right_eye']\n",
    "        return np.sqrt((left_eye[0] - right_eye[0])**2 + \n",
    "                      (left_eye[1] - right_eye[1])**2)\n",
    "        \n",
    "    def calculate_mouth_width(self, keypoints):\n",
    "        \"\"\"Calculate mouth width\"\"\"\n",
    "        mouth_left = keypoints['mouth_left']\n",
    "        mouth_right = keypoints['mouth_right']\n",
    "        return np.sqrt((mouth_left[0] - mouth_right[0])**2 + \n",
    "                      (mouth_left[1] - mouth_right[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_temporal_features(self, frame_features):\n",
    "        \"\"\"Extract temporal features across frames\"\"\"\n",
    "        if len(frame_features) < 2:\n",
    "            return None\n",
    "            \n",
    "        temporal_features = {\n",
    "            'confidence_mean': np.mean([f['confidence'] for f in frame_features]),\n",
    "            'movement': self.calculate_movement(frame_features),\n",
    "            'expression_change': self.calculate_expression_change(frame_features)\n",
    "        }\n",
    "        \n",
    "        return temporal_features\n",
    "        \n",
    "    def calculate_movement(self, frame_features):\n",
    "        \"\"\"Calculate face movement between frames\"\"\"\n",
    "        movements = []\n",
    "        for i in range(1, len(frame_features)):\n",
    "            prev = frame_features[i-1]['box']\n",
    "            curr = frame_features[i]['box']\n",
    "            \n",
    "            prev_center = [prev[0] + prev[2]/2, prev[1] + prev[3]/2]\n",
    "            curr_center = [curr[0] + curr[2]/2, curr[1] + curr[3]/2]\n",
    "            \n",
    "            movement = np.sqrt((prev_center[0] - curr_center[0])**2 + \n",
    "                             (prev_center[1] - curr_center[1])**2)\n",
    "            movements.append(movement)\n",
    "            \n",
    "        return np.mean(movements) if movements else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureIntegrator:\n",
    "    def __init__(self):\n",
    "        self.video_processor = VideoProcessor()\n",
    "        \n",
    "    def process_video(self, video_path):\n",
    "        \"\"\"Process video and extract all features\"\"\"\n",
    "        # Load and process video\n",
    "        frames = self.video_processor.load_video(video_path)\n",
    "        \n",
    "        # Extract features from each frame\n",
    "        frame_features = []\n",
    "        for frame in frames:\n",
    "            features = self.video_processor.detect_faces(frame)\n",
    "            if features is not None:\n",
    "                frame_features.append(features)\n",
    "                \n",
    "        # Extract temporal features\n",
    "        temporal_features = self.video_processor.extract_temporal_features(frame_features)\n",
    "        \n",
    "        return {\n",
    "            'frame_features': frame_features,\n",
    "            'temporal_features': temporal_features\n",
    "        }\n",
    "        \n",
    "    def combine_features(self, video_features, text_features):\n",
    "        \"\"\"Combine video and text features\"\"\"\n",
    "        # Normalize features by duration\n",
    "        duration = text_features['utterance_duration']\n",
    "        \n",
    "        combined_features = {\n",
    "            # Video features\n",
    "            'visual_confidence': np.mean([f['confidence'] for f in video_features['frame_features']]),\n",
    "            'movement_amount': video_features['temporal_features']['movement'] / duration,\n",
    "            'expression_change': video_features['temporal_features']['expression_change'] / duration,\n",
    "            \n",
    "            # Text features\n",
    "            'text_sentiment': text_features['sentiment'],\n",
    "            'speaking_rate': text_features['speaking_rate'],\n",
    "            \n",
    "            # Temporal features\n",
    "            'duration': duration,\n",
    "            'pause_after': text_features['pause_after']\n",
    "        }\n",
    "        \n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Audio Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install setuptools==57.5.0\n",
    "%pip install praat-parselmouth\n",
    "%pip install librosa\n",
    "%pip install soundfile  parselmouth tqdm pandas\n",
    "%pip uninstall moviepy\n",
    "%pip install moviepy==1.0.3\n",
    "\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load training and test data with proper encoding\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv('train/train_prop_sent_csv3_final.csv', encoding='ISO-8859-1')\n",
    "        test_df = pd.read_csv('test/test_prop_sent_csv3_final.csv', encoding='ISO-8859-1')\n",
    "        \n",
    "        # Add video paths\n",
    "        train_df['video_clip_path'] = train_df.apply(lambda x: f\"train/train_prop_sent_data3/dia{x['Dialogue_ID']}_utt{x['Utterance_ID']}.mp4\", axis=1)\n",
    "        test_df['video_clip_path'] = test_df.apply(lambda x: f\"test/test_prop_sent_data3/dia{x['Dialogue_ID']}_utt{x['Utterance_ID']}.mp4\", axis=1)\n",
    "        \n",
    "        return train_df, test_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=16000, temp_dir='temp_audio'):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.temp_dir = temp_dir\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "    def extract_audio_from_video(self, video_path):\n",
    "        \"\"\"Extract audio from video file\"\"\"\n",
    "        try:\n",
    "            temp_path = os.path.join(self.temp_dir, f\"temp_{os.path.basename(video_path)}.wav\")\n",
    "            if not os.path.exists(temp_path):\n",
    "                video = VideoFileClip(video_path)\n",
    "                audio = video.audio\n",
    "                if audio is None:\n",
    "                    return None, None\n",
    "                audio.write_audiofile(temp_path, logger=None)\n",
    "                video.close()\n",
    "            y, sr = librosa.load(temp_path, sr=self.sample_rate)\n",
    "            return y, sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_path}: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def extract_features(self, y, sr):\n",
    "        \"\"\"Extract all audio features\"\"\"\n",
    "        try:\n",
    "            # Basic features\n",
    "            sound = parselmouth.Sound(y, sr)\n",
    "            pitch = sound.to_pitch()\n",
    "            pitch_values = pitch.selected_array['frequency']\n",
    "            pitch_values = pitch_values[pitch_values != 0]\n",
    "            \n",
    "            # Energy features\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            \n",
    "            # Voice quality\n",
    "            harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "            hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "            \n",
    "            return {\n",
    "                'pitch_mean': float(np.mean(pitch_values)) if len(pitch_values) > 0 else 0,\n",
    "                'pitch_std': float(np.std(pitch_values)) if len(pitch_values) > 0 else 0,\n",
    "                'energy_mean': float(np.mean(rms)),\n",
    "                'energy_std': float(np.std(rms)), \n",
    "                'hnr': float(hnr)\n",
    "            }\n",
    "            \n",
    "        except:\n",
    "            return {k:0 for k in ['pitch_mean', 'pitch_std', 'energy_mean', 'energy_std', 'hnr']}\n",
    "\n",
    "    def process_audio(self, video_path):\n",
    "        \"\"\"Process single video and extract features\"\"\"\n",
    "        y, sr = self.extract_audio_from_video(video_path)\n",
    "        if y is None:\n",
    "            return None\n",
    "        return self.extract_features(y, sr)\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up temp files\"\"\"\n",
    "        import shutil\n",
    "        if os.path.exists(self.temp_dir):\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "\n",
    "def process_dataset(df, batch_size=32):\n",
    "    \"\"\"Process audio features for entire dataset\"\"\"\n",
    "    audio_processor = AudioProcessor()\n",
    "    all_features = []\n",
    "    \n",
    "    try:\n",
    "        for i in tqdm(range(0, len(df), batch_size)):\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            batch_features = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                features = audio_processor.process_audio(row['video_clip_path'])\n",
    "                if features is None:\n",
    "                    features = {k:0 for k in ['pitch_mean', 'pitch_std', 'energy_mean', 'energy_std', 'hnr']}\n",
    "                batch_features.append(features)\n",
    "            all_features.extend(batch_features)\n",
    "    finally:\n",
    "        audio_processor.cleanup()\n",
    "        \n",
    "    return pd.DataFrame(all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "class FeatureIntegrator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize feature integrator\"\"\"\n",
    "        self.text_features = None\n",
    "        self.video_features = None \n",
    "        self.audio_features = None\n",
    "        \n",
    "    def extract_all_features(self, df):\n",
    "        \"\"\"Extract features from all modalities\"\"\"\n",
    "        # Calculate duration and speaking rate\n",
    "        df['start_time'] = pd.to_datetime(df['StartTime'])\n",
    "        df['end_time'] = pd.to_datetime(df['EndTime'])\n",
    "        df['utterance_duration'] = (df['end_time'] - df['start_time']).dt.total_seconds()\n",
    "        df['word_count'] = df['Utterance'].str.split().str.len()\n",
    "        df['speaking_rate'] = df['word_count'] / df['utterance_duration']\n",
    "        \n",
    "        # Text Features\n",
    "        text_features = {\n",
    "            'text_sentiment': df['Sentiment'],\n",
    "            'exclamation_count': df['Utterance'].str.count('!'),\n",
    "            'question_count': df['Utterance'].str.count(r'\\?'),\n",
    "            'ellipsis_count': df['Utterance'].str.count(r'\\.\\.\\.'),\n",
    "            'word_count': df['word_count'],\n",
    "            'speaking_rate': df['speaking_rate']\n",
    "        }\n",
    "        \n",
    "        # Speaker Features\n",
    "        speaker_features = {\n",
    "            'speaker_total_utterances': df.groupby('Speaker')['Utterance'].transform('count'),\n",
    "            'speaker_sentiment_ratio': df.groupby('Speaker')['Sentiment'].transform(\n",
    "                lambda x: (x == 'positive').mean()\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Context Features\n",
    "        context_features = {\n",
    "            'prev_sentiment': df.groupby('Dialogue_ID')['Sentiment'].shift(1),\n",
    "            'prev_speaker': df.groupby('Dialogue_ID')['Speaker'].shift(1),\n",
    "            'relative_position': df.groupby('Dialogue_ID').cumcount() / \n",
    "                               df.groupby('Dialogue_ID').size()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'text': text_features,\n",
    "            'speaker': speaker_features,\n",
    "            'context': context_features\n",
    "        }\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Create base models\n",
    "        self.model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb1', xgb.XGBClassifier(\n",
    "                    n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "                    subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "                    use_label_encoder=False, eval_metric='mlogloss'\n",
    "                )),\n",
    "                ('xgb2', xgb.XGBClassifier(\n",
    "                    n_estimators=200, max_depth=7, learning_rate=0.05,\n",
    "                    subsample=0.7, colsample_bytree=0.7, random_state=43,\n",
    "                    use_label_encoder=False, eval_metric='mlogloss'\n",
    "                ))\n",
    "            ],\n",
    "            weights=[0.6, 0.4],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare features for training/prediction\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Text features\n",
    "        features.extend([\n",
    "            df['Utterance'].str.count('!'),\n",
    "            df['Utterance'].str.count(r'\\?'),\n",
    "            df['Utterance'].str.count(r'\\.\\.\\.'),\n",
    "            df['Utterance'].str.split().str.len(),\n",
    "            df['Utterance'].str.len()\n",
    "        ])\n",
    "        \n",
    "        # Speaker features\n",
    "        features.extend([\n",
    "            pd.Categorical(df['Speaker']).codes,\n",
    "            df.groupby('Speaker')['Utterance'].transform('count')\n",
    "        ])\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X = np.column_stack(features)\n",
    "        return X\n",
    "        \n",
    "    def train_with_cv(self, train_df, n_splits=5):\n",
    "        \"\"\"Train with cross-validation\"\"\"\n",
    "        X = self.prepare_features(train_df)\n",
    "        y = train_df['Sentiment'].values\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        val_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            X_train = self.scaler.fit_transform(X_train)\n",
    "            X_val = self.scaler.transform(X_val)\n",
    "            \n",
    "            self.model.fit(X_train, y_train)\n",
    "            val_pred = self.model.predict(X_val)\n",
    "            \n",
    "            print(f\"\\nFold {fold + 1} Validation Report:\")\n",
    "            print(classification_report(y_val, val_pred))\n",
    "            val_scores.append(classification_report(y_val, val_pred, output_dict=True)['weighted avg']['f1-score'])\n",
    "            \n",
    "        print(f\"\\nAverage F1 Score: {np.mean(val_scores):.3f}\")\n",
    "        \n",
    "        # Train final model\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "    def predict(self, test_df):\n",
    "        \"\"\"Generate predictions for test data\"\"\"\n",
    "        X_test = self.prepare_features(test_df)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv('train/train_prop_sent_csv3_final.csv', encoding='ISO-8859-1')\n",
    "    test_df = pd.read_csv('test/test_prop_sent_csv3_final.csv', encoding='ISO-8859-1')\n",
    "    \n",
    "    # Process features and train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    classifier = SentimentClassifier()\n",
    "    classifier.train_with_cv(train_df)\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    predictions = classifier.predict(test_df)\n",
    "    \n",
    "    # Create submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Sr No.': test_df['Sr No.'],\n",
    "        'Sentiment': predictions\n",
    "    })\n",
    "    \n",
    "    # Save submission\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nSubmission saved!\")\n",
    "    \n",
    "    # Print distribution\n",
    "    print(\"\\nPredicted class distribution:\")\n",
    "    print(pd.Series(predictions).value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Model\n",
    "class TextModel:\n",
    "    def __init__(self):\n",
    "        self.text_features = [\n",
    "            'exclamation_count', 'question_count', 'ellipsis_count',\n",
    "            'speaking_rate', 'utterance_duration', 'pause_after',\n",
    "            'speaker_positive_ratio', 'speaker_negative_ratio'\n",
    "        ]\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        return df[self.text_features].values\n",
    "\n",
    "# Video Model\n",
    "class VideoModel:\n",
    "    def __init__(self):\n",
    "        self.video_features = [\n",
    "            'face_confidence', 'expression_intensity', 'movement_amount',\n",
    "            'expression_changes'\n",
    "        ]\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        return df[self.video_features].values\n",
    "\n",
    "# Audio Model\n",
    "class AudioModel:\n",
    "    def __init__(self):\n",
    "        self.audio_features = [\n",
    "            'pitch_mean', 'energy_mean', 'speech_rate',\n",
    "            'hnr', 'jitter', 'shimmer'\n",
    "        ]\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        return df[self.audio_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyFusionModel:\n",
    "    def __init__(self):\n",
    "        self.text_model = TextModel()\n",
    "        self.video_model = VideoModel()\n",
    "        self.audio_model = AudioModel()\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        # Combine features from all modalities\n",
    "        text_features = self.text_model.prepare_features(df)\n",
    "        video_features = self.video_model.prepare_features(df)\n",
    "        audio_features = self.audio_model.prepare_features(df)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate([\n",
    "            text_features,\n",
    "            video_features,\n",
    "            audio_features\n",
    "        ], axis=1)\n",
    "        \n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusionModel:\n",
    "    def __init__(self):\n",
    "        self.text_model = TextModel()\n",
    "        self.video_model = VideoModel()\n",
    "        self.audio_model = AudioModel()\n",
    "        \n",
    "        # Individual classifiers for each modality\n",
    "        self.text_classifier = None\n",
    "        self.video_classifier = None\n",
    "        self.audio_classifier = None\n",
    "        \n",
    "    def train_individual_models(self, df):\n",
    "        # Train text model\n",
    "        text_features = self.text_model.prepare_features(df)\n",
    "        self.text_classifier.fit(text_features, df['Sentiment'])\n",
    "        \n",
    "        # Train video model\n",
    "        video_features = self.video_model.prepare_features(df)\n",
    "        self.video_classifier.fit(video_features, df['Sentiment'])\n",
    "        \n",
    "        # Train audio model\n",
    "        audio_features = self.audio_model.prepare_features(df)\n",
    "        self.audio_classifier.fit(audio_features, df['Sentiment'])\n",
    "        \n",
    "    def predict(self, df):\n",
    "        # Get predictions from each model\n",
    "        text_pred = self.text_classifier.predict_proba(\n",
    "            self.text_model.prepare_features(df)\n",
    "        )\n",
    "        video_pred = self.video_classifier.predict_proba(\n",
    "            self.video_model.prepare_features(df)\n",
    "        )\n",
    "        audio_pred = self.audio_classifier.predict_proba(\n",
    "            self.audio_model.prepare_features(df)\n",
    "        )\n",
    "        \n",
    "        # Combine predictions (e.g., weighted average)\n",
    "        final_pred = (0.4 * text_pred + \n",
    "                     0.3 * video_pred + \n",
    "                     0.3 * audio_pred)\n",
    "        \n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFusionModel:\n",
    "    def __init__(self):\n",
    "        self.text_model = TextModel()\n",
    "        self.video_model = VideoModel()\n",
    "        self.audio_model = AudioModel()\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        # Low-level feature fusion\n",
    "        text_features = self.text_model.prepare_features(df)\n",
    "        video_features = self.video_model.prepare_features(df)\n",
    "        \n",
    "        # Combine text and video features\n",
    "        text_video_features = np.concatenate([\n",
    "            text_features,\n",
    "            video_features\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Train intermediate model\n",
    "        intermediate_pred = self.intermediate_classifier.predict_proba(\n",
    "            text_video_features\n",
    "        )\n",
    "        \n",
    "        # Get audio predictions\n",
    "        audio_features = self.audio_model.prepare_features(df)\n",
    "        audio_pred = self.audio_classifier.predict_proba(\n",
    "            audio_features\n",
    "        )\n",
    "        \n",
    "        # Late fusion of intermediate and audio predictions\n",
    "        final_pred = 0.7 * intermediate_pred + 0.3 * audio_pred\n",
    "        \n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_df):\n",
    "    \"\"\"\n",
    "    Train model on training data\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    X_train = model.prepare_features(train_df)\n",
    "    y_train = train_df['Sentiment']\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_test(model, test_df):\n",
    "    \"\"\"\n",
    "    Make predictions on test data\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    X_test = model.prepare_features(test_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(model, val_df):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on validation set\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    X_val = model.prepare_features(val_df)\n",
    "    y_val = val_df['Sentiment']\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Create base models\n",
    "        self.xgb1 = xgb.XGBClassifier(\n",
    "            n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, use_label_encoder=False, eval_metric='mlogloss'\n",
    "        )\n",
    "        \n",
    "        self.xgb2 = xgb.XGBClassifier(\n",
    "            n_estimators=200, max_depth=7, learning_rate=0.05,\n",
    "            subsample=0.7, colsample_bytree=0.7,\n",
    "            random_state=43, use_label_encoder=False, eval_metric='mlogloss'\n",
    "        )\n",
    "        \n",
    "        self.xgb3 = xgb.XGBClassifier(\n",
    "            n_estimators=200, max_depth=9, learning_rate=0.01,\n",
    "            subsample=0.6, colsample_bytree=0.6,\n",
    "            random_state=44, use_label_encoder=False, eval_metric='mlogloss'\n",
    "        )\n",
    "        \n",
    "        # Create weighted ensemble\n",
    "        self.model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb1', self.xgb1),\n",
    "                ('xgb2', self.xgb2),\n",
    "                ('xgb3', self.xgb3)\n",
    "            ],\n",
    "            weights=[0.4, 0.3, 0.3],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare features for training/prediction\"\"\"\n",
    "        # Basic text features\n",
    "        df['exclamation_count'] = df['Utterance'].str.count('!')\n",
    "        df['question_count'] = df['Utterance'].str.count(r'\\?')\n",
    "        df['ellipsis_count'] = df['Utterance'].str.count(r'\\.\\.\\.')\n",
    "        \n",
    "        # Word-level features\n",
    "        df['word_count'] = df['Utterance'].str.split().str.len()\n",
    "        df['char_count'] = df['Utterance'].str.len()\n",
    "        df['avg_word_length'] = df['Utterance'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if pd.notnull(x) else 0)\n",
    "        \n",
    "        # Select features for model\n",
    "        feature_cols = [\n",
    "            'exclamation_count', 'question_count', 'ellipsis_count',\n",
    "            'word_count', 'char_count', 'avg_word_length'\n",
    "        ]\n",
    "        \n",
    "        return df[feature_cols].values\n",
    "        \n",
    "    def train_with_cv(self, train_df, n_splits=5):\n",
    "        \"\"\"Train with cross-validation\"\"\"\n",
    "        # Reset index to avoid indexing issues\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        \n",
    "        X = self.prepare_features(train_df)\n",
    "        y = train_df['Sentiment'].values  # Convert to numpy array\n",
    "        \n",
    "        # Initialize stratified k-fold\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Store validation scores\n",
    "        val_scores = []\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            # Split data\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            X_train = self.scaler.fit_transform(X_train)\n",
    "            X_val = self.scaler.transform(X_val)\n",
    "            \n",
    "            # Train model\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "            # Validate\n",
    "            val_pred = self.model.predict(X_val)\n",
    "            val_score = classification_report(y_val, val_pred, output_dict=True)\n",
    "            val_scores.append(val_score['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"\\nFold {fold + 1} Validation Report:\")\n",
    "            print(classification_report(y_val, val_pred))\n",
    "            \n",
    "        print(f\"\\nAverage F1 Score across folds: {np.mean(val_scores):.3f}\")\n",
    "        \n",
    "        # Train final model on full dataset\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, y)\n",
    "            \n",
    "    def predict(self, test_df):\n",
    "        \"\"\"Generate predictions for test data\"\"\"\n",
    "        X_test = self.prepare_features(test_df)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Train model with cross-validation\n",
    "print(\"Training model with cross-validation...\")\n",
    "classifier = SentimentClassifier()\n",
    "classifier.train_with_cv(train_df)\n",
    "\n",
    "# Make predictions on test data\n",
    "print(\"\\nGenerating predictions for test data...\")\n",
    "test_predictions = classifier.predict(test_df)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Sr No.': test_df['Sr No.'],\n",
    "    'Sentiment': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file created!\")\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nPredicted class distribution:\")\n",
    "print(pd.Series(test_predictions).value_counts())\n",
    "\n",
    "# Print sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define path to video clips\n",
    "df = pd.read_csv('test/test_prop_sent_csv3_final.csv', encoding='ISO-8859-1')\n",
    "video_dir = 'test/test_prop_sent_data3'\n",
    "\n",
    "\n",
    "# Function to get video file path from IDs\n",
    "def get_video_clip_path(row):\n",
    "    dialogue_id = row['Dialogue_ID']\n",
    "    utterance_id = row['Utterance_ID']\n",
    "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
    "    return os.path.join(video_dir, filename)\n",
    "\n",
    "# Apply the function to get file paths for each sampled clip\n",
    "df['video_clip_path'] = df.apply(get_video_clip_path, axis=1)\n",
    "\n",
    "# Check sample paths\n",
    "print(df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_preds = [\"your_prediction\" for i in df['Utterance_ID']]\n",
    "all_ids = df[\"Sr No.\"]\n",
    "submission_df = pd.DataFrame({\n",
    "        'Sr No.': all_ids,\n",
    "        'Emotion': all_preds\n",
    "    })\n",
    "    \n",
    "# Save the DataFrame to CSV\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10107338,
     "sourceId": 88291,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
